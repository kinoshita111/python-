{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a72IB5-lKWyJ",
        "outputId": "26858cd2-5f5e-494f-d884-e9acfd9f8b72"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 24 04:34:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrKV-95jKmx2",
        "outputId": "b9d5ce1c-2dcd-464e-ef59-33663210647f"
      },
      "source": [
        "!wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-24 04:34:21--  http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
            "Resolving www.phontron.com (www.phontron.com)... 208.113.196.149\n",
            "Connecting to www.phontron.com (www.phontron.com)|208.113.196.149|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99246893 (95M) [application/gzip]\n",
            "Saving to: ‘kftt-data-1.0.tar.gz’\n",
            "\n",
            "kftt-data-1.0.tar.g 100%[===================>]  94.65M  37.2MB/s    in 2.5s    \n",
            "\n",
            "2021-02-24 04:34:24 (37.2 MB/s) - ‘kftt-data-1.0.tar.gz’ saved [99246893/99246893]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N3e8Z63KqwC",
        "outputId": "96a36e4b-c4e3-4a6e-8d59-56e014f576c5"
      },
      "source": [
        "!tar -zxvf /content/kftt-data-1.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kftt-data-1.0/\n",
            "kftt-data-1.0/data/\n",
            "kftt-data-1.0/data/orig/\n",
            "kftt-data-1.0/data/orig/kyoto-tune.en\n",
            "kftt-data-1.0/data/orig/kyoto-dev.ja\n",
            "kftt-data-1.0/data/orig/kyoto-dev.en\n",
            "kftt-data-1.0/data/orig/kyoto-train.en\n",
            "kftt-data-1.0/data/orig/kyoto-tune.ja\n",
            "kftt-data-1.0/data/orig/kyoto-train.ja\n",
            "kftt-data-1.0/data/orig/kyoto-test.ja\n",
            "kftt-data-1.0/data/orig/kyoto-test.en\n",
            "kftt-data-1.0/data/tok/\n",
            "kftt-data-1.0/data/tok/kyoto-tune.en\n",
            "kftt-data-1.0/data/tok/kyoto-dev.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.cln.en\n",
            "kftt-data-1.0/data/tok/kyoto-dev.en\n",
            "kftt-data-1.0/data/tok/kyoto-train.en\n",
            "kftt-data-1.0/data/tok/kyoto-tune.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.cln.ja\n",
            "kftt-data-1.0/data/tok/kyoto-train.ja\n",
            "kftt-data-1.0/data/tok/kyoto-test.ja\n",
            "kftt-data-1.0/data/tok/kyoto-test.en\n",
            "kftt-data-1.0/README.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwbmIBBvLeQW",
        "outputId": "f7425428-2e57-4cb4-9c0b-d38c67e6a3ed"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BCLezsSKwDa"
      },
      "source": [
        "!cp -r kftt-data-1.0/data/tok/ 'drive/My Drive/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbQGgTSAKzh8",
        "outputId": "41e4adf3-167d-4793-dfac-9b2ffeb5a76c"
      },
      "source": [
        "!ls '/content/drive/My Drive/tok'\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kyoto-dev.en   kyoto-train.cln.en  kyoto-tune.en       limit-kyoto-test.en\n",
            "kyoto-dev.ja   kyoto-train.cln.ja  kyoto-tune.ja       limit-kyoto-test.ja\n",
            "kyoto-test.en  kyoto-train.en\t   limit-kyoto-dev.en  limit-kyoto-train.en\n",
            "kyoto-test.ja  kyoto-train.ja\t   limit-kyoto-dev.ja  limit-kyoto-train.ja\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTKFiYpdL07I",
        "outputId": "375a82f4-f9c1-474c-e9fb-e34637d7893f"
      },
      "source": [
        "%cd drive/My Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEq5fH5TL3ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1820ac9d-b8ca-4d71-e802-cf932aabbeb8"
      },
      "source": [
        "!mkdir data\r\n",
        "!mkdir save"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘save’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPgz8cerL_Tc"
      },
      "source": [
        "def limiter(data_dir, src_data, tgt_data, n=50):\r\n",
        "  '''limiter\r\n",
        "  コーパスの最大文長を制限する。\r\n",
        "  ----------------------------------------\r\n",
        "  引数\r\n",
        "  data_dir : コーパスのディレクトリ\r\n",
        "  src_data : 原言語のデータ\r\n",
        "  tgt_data : 目的言語のデータ\r\n",
        "  n : 最大文長\r\n",
        "  ----------------------------------------\r\n",
        "  '''\r\n",
        "  # 書き込み用のデータファイルを作成する。\r\n",
        "  with open(f'{data_dir}/limit-{src_data}', 'w', encoding='utf8') as wsrc:\r\n",
        "    with open(f'{data_dir}/limit-{tgt_data}', 'w', encoding='utf8') as wtgt:\r\n",
        "      # 元のデータファイルを読み込む\r\n",
        "      with open(f'{data_dir}/{src_data}', encoding='utf8') as rsrc:\r\n",
        "        with open(f'{data_dir}/{tgt_data}', encoding='utf8') as rtgt:\r\n",
        "          # それぞれ一行ずつ読み込みリストに格納する。\r\n",
        "          src_lines = rsrc.read().strip().split('\\n')\r\n",
        "          tgt_lines = rtgt.read().strip().split('\\n')\r\n",
        "          for src, tgt in zip(src_lines, tgt_lines):\r\n",
        "            # 最大文長がn以下のもののみ書き込む。\r\n",
        "            if len(src.split()) <= n and len(tgt.split()) <= n:\r\n",
        "              wsrc.write(src + '\\n')\r\n",
        "              wtgt.write(tgt + '\\n')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I50L0MMMFo9"
      },
      "source": [
        "## 文長制限するファイルの一覧\r\n",
        "files = ['train', 'dev', 'test']\r\n",
        "for f in files:\r\n",
        "  # 最大文長10とする\r\n",
        "  limiter('tok', f'kyoto-{f}.en', f'kyoto-{f}.ja', n=10)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHSQKjE8MJg5",
        "outputId": "ab7ccc7f-a4d7-4ee4-a9da-27a23cb821c1"
      },
      "source": [
        "!ls tok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kyoto-dev.en   kyoto-train.cln.en  kyoto-tune.en       limit-kyoto-test.en\n",
            "kyoto-dev.ja   kyoto-train.cln.ja  kyoto-tune.ja       limit-kyoto-test.ja\n",
            "kyoto-test.en  kyoto-train.en\t   limit-kyoto-dev.en  limit-kyoto-train.en\n",
            "kyoto-test.ja  kyoto-train.ja\t   limit-kyoto-dev.ja  limit-kyoto-train.ja\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ek9IA-hMNQC",
        "outputId": "106a1662-1179-4cd7-a489-0b421e11b446"
      },
      "source": [
        "# 文長制限ができているか確認する。\r\n",
        "!wc -l tok/limit-kyoto-train.en\r\n",
        "!wc -l tok/limit-kyoto-train.ja"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71683 tok/limit-kyoto-train.en\n",
            "71683 tok/limit-kyoto-train.ja\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAvtPiWVMR88",
        "outputId": "b9e5817e-399d-44f5-801f-ec9913317d2a"
      },
      "source": [
        "!pip install OpenNMT-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting OpenNMT-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/62/2c50d622c24cdce54523ec64051511793661ec14d396e05875597befa00d/OpenNMT_py-2.0.1-py3-none-any.whl (207kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20kB 28.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40kB 29.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 51kB 26.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 61kB 29.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71kB 18.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 81kB 20.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 92kB 18.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 102kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 112kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 122kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 133kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 143kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 153kB 18.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 163kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 174kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 184kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 194kB 18.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 204kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.4.1)\n",
            "Collecting torch==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/5e/35140615fc1f925023f489e71086a9ecc188053d263d3594237281284d82/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8MB 24kB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.23; platform_system == \"Linux\" or platform_system == \"Darwin\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/aa/eaf0eaee095a5d42b6ed9fa969c8a37085e059f71efb1e55cb220e245437/pyonmttok-1.24.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 46.0MB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 13.3MB/s \n",
            "\u001b[?25hCollecting waitress==1.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/d1/5209fb8c764497a592363c47054436a515b47b8c3e4970ddd7184f088857/waitress-1.4.4-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 11.1MB/s \n",
            "\u001b[?25hCollecting configargparse<2,>=1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/75/ca907906cd6c4c7097a037f7adaee36b3f32a08b66baed51b86d1fcc6398/ConfigArgParse-1.3.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
            "\u001b[?25hCollecting pyyaml==5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask==1.1.2 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.1.2)\n",
            "Collecting tqdm<5,>=4.51\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/13/f3f815bb73804a8af9cfbb6f084821c037109108885f46131045e8cf044e/tqdm-4.57.0-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.36.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.27.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (53.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (3.3.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.32.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py) (1.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->OpenNMT-py) (0.16.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py) (7.1.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (4.7.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3->OpenNMT-py) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask==1.1.2->OpenNMT-py) (1.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py) (3.4.0)\n",
            "Building wheels for collected packages: configargparse, pyyaml\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-1.3-cp37-none-any.whl size=19478 sha256=e7fa5556be67a38214d26f35e636823e9a5a66d0654008c7e1942bc472d35270\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/d5/5f/3001db0714a92f771c292603ef5dada52f9efa6467f3ea2bdf\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=f6f3c13772309053aca5a85006fb1a0463623184bb050997dc85824b7447a0bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built configargparse pyyaml\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, pyonmttok, sentencepiece, tqdm, torchtext, waitress, configargparse, pyyaml, OpenNMT-py\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed OpenNMT-py-2.0.1 configargparse-1.3 pyonmttok-1.24.0 pyyaml-5.3.1 sentencepiece-0.1.95 torch-1.6.0 torchtext-0.5.0 tqdm-4.57.0 waitress-1.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0A_VNKiMXJX",
        "outputId": "b297cc16-a155-40c8-93aa-f01c5b5e658c"
      },
      "source": [
        "# データの前処理をする。\r\n",
        "!onmt_preprocess -train_src tok/limit-kyoto-train.en \\\r\n",
        "                 -train_tgt tok/limit-kyoto-train.ja \\\r\n",
        "                 -valid_src tok/limit-kyoto-dev.en \\\r\n",
        "                 -valid_tgt tok/limit-kyoto-dev.ja \\\r\n",
        "                 -save_data data/demo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: onmt_preprocess: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzWiTZauMa0T",
        "outputId": "97d80bbd-7d60-48c6-dde6-908c1786e1cb"
      },
      "source": [
        "!ls data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "demo.train.0.pt  demo.valid.0.pt  demo.vocab.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCHkHSEMMnV7",
        "outputId": "2bea61ef-cc4f-47dd-a81b-3c689d2a5670"
      },
      "source": [
        "!onmt_train -data data/demo \\\r\n",
        "            -save_model save \\\r\n",
        "            -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\\r\n",
        "            -encoder_type transformer -decoder_type transformer -position_encoding \\\r\n",
        "            -train_steps 10000  -max_generator_batches 2 -dropout 0.1 \\\r\n",
        "            -batch_size 1024 -batch_type tokens -normalization tokens  -accum_count 2 \\\r\n",
        "            -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 4000 -learning_rate 2 \\\r\n",
        "            -max_grad_norm 0 -param_init 0  -param_init_glorot \\\r\n",
        "            -label_smoothing 0.1 -valid_steps 2000 -save_checkpoint_steps 2000 \\\r\n",
        "            -world_size 1 -gpu_ranks 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: onmt_train [-h] [-config CONFIG] [-save_config SAVE_CONFIG] -data DATA\n",
            "                  [-skip_empty_level {silent,warning,error}]\n",
            "                  [-transforms {sentencepiece,bpe,onmt_tokenize,filtertoolong,prefix,switchout,tokendrop,tokenmask,bart} [{sentencepiece,bpe,onmt_tokenize,filtertoolong,prefix,switchout,tokendrop,tokenmask,bart} ...]]\n",
            "                  [-save_data SAVE_DATA] [-overwrite] [-n_sample N_SAMPLE]\n",
            "                  [-dump_fields] [-dump_transforms] -src_vocab SRC_VOCAB\n",
            "                  [-tgt_vocab TGT_VOCAB] [-share_vocab]\n",
            "                  [-src_vocab_size SRC_VOCAB_SIZE]\n",
            "                  [-tgt_vocab_size TGT_VOCAB_SIZE]\n",
            "                  [-vocab_size_multiple VOCAB_SIZE_MULTIPLE]\n",
            "                  [-src_words_min_frequency SRC_WORDS_MIN_FREQUENCY]\n",
            "                  [-tgt_words_min_frequency TGT_WORDS_MIN_FREQUENCY]\n",
            "                  [--src_seq_length_trunc SRC_SEQ_LENGTH_TRUNC]\n",
            "                  [--tgt_seq_length_trunc TGT_SEQ_LENGTH_TRUNC]\n",
            "                  [-both_embeddings BOTH_EMBEDDINGS]\n",
            "                  [-src_embeddings SRC_EMBEDDINGS]\n",
            "                  [-tgt_embeddings TGT_EMBEDDINGS]\n",
            "                  [-embeddings_type {GloVe,word2vec}]\n",
            "                  [-src_subword_model SRC_SUBWORD_MODEL]\n",
            "                  [-tgt_subword_model TGT_SUBWORD_MODEL]\n",
            "                  [-src_subword_nbest SRC_SUBWORD_NBEST]\n",
            "                  [-tgt_subword_nbest TGT_SUBWORD_NBEST]\n",
            "                  [-src_subword_alpha SRC_SUBWORD_ALPHA]\n",
            "                  [-tgt_subword_alpha TGT_SUBWORD_ALPHA]\n",
            "                  [-src_subword_vocab SRC_SUBWORD_VOCAB]\n",
            "                  [-tgt_subword_vocab TGT_SUBWORD_VOCAB]\n",
            "                  [-src_vocab_threshold SRC_VOCAB_THRESHOLD]\n",
            "                  [-tgt_vocab_threshold TGT_VOCAB_THRESHOLD]\n",
            "                  [-src_subword_type {none,sentencepiece,bpe}]\n",
            "                  [-tgt_subword_type {none,sentencepiece,bpe}]\n",
            "                  [-src_onmttok_kwargs SRC_ONMTTOK_KWARGS]\n",
            "                  [-tgt_onmttok_kwargs TGT_ONMTTOK_KWARGS]\n",
            "                  [--src_seq_length SRC_SEQ_LENGTH]\n",
            "                  [--tgt_seq_length TGT_SEQ_LENGTH]\n",
            "                  [-switchout_temperature SWITCHOUT_TEMPERATURE]\n",
            "                  [-tokendrop_temperature TOKENDROP_TEMPERATURE]\n",
            "                  [-tokenmask_temperature TOKENMASK_TEMPERATURE]\n",
            "                  [--permute_sent_ratio PERMUTE_SENT_RATIO]\n",
            "                  [--rotate_ratio ROTATE_RATIO] [--insert_ratio INSERT_RATIO]\n",
            "                  [--random_ratio RANDOM_RATIO] [--mask_ratio MASK_RATIO]\n",
            "                  [--mask_length {subword,word,span-poisson}]\n",
            "                  [--poisson_lambda POISSON_LAMBDA]\n",
            "                  [--replace_length {-1,0,1}]\n",
            "                  [--src_word_vec_size SRC_WORD_VEC_SIZE]\n",
            "                  [--tgt_word_vec_size TGT_WORD_VEC_SIZE]\n",
            "                  [--word_vec_size WORD_VEC_SIZE] [--share_decoder_embeddings]\n",
            "                  [--share_embeddings] [--position_encoding]\n",
            "                  [--feat_merge {concat,sum,mlp}]\n",
            "                  [--feat_vec_size FEAT_VEC_SIZE]\n",
            "                  [--feat_vec_exponent FEAT_VEC_EXPONENT]\n",
            "                  [-model_task {seq2seq,lm}] [--model_type {text}]\n",
            "                  [--model_dtype {fp32,fp16}]\n",
            "                  [--encoder_type {rnn,brnn,ggnn,mean,transformer,cnn,transformer_lm}]\n",
            "                  [--decoder_type {rnn,transformer,cnn,transformer_lm}]\n",
            "                  [--layers LAYERS] [--enc_layers ENC_LAYERS]\n",
            "                  [--dec_layers DEC_LAYERS] [--rnn_size RNN_SIZE]\n",
            "                  [--enc_rnn_size ENC_RNN_SIZE] [--dec_rnn_size DEC_RNN_SIZE]\n",
            "                  [--cnn_kernel_width CNN_KERNEL_WIDTH]\n",
            "                  [--pos_ffn_activation_fn {relu,gelu}]\n",
            "                  [--input_feed INPUT_FEED] [--bridge]\n",
            "                  [--rnn_type {LSTM,GRU,SRU}] [--brnn]\n",
            "                  [--context_gate {source,target,both}]\n",
            "                  [--bridge_extra_node BRIDGE_EXTRA_NODE]\n",
            "                  [--bidir_edges BIDIR_EDGES] [--state_dim STATE_DIM]\n",
            "                  [--n_edge_types N_EDGE_TYPES] [--n_node N_NODE]\n",
            "                  [--n_steps N_STEPS] [--src_ggnn_size SRC_GGNN_SIZE]\n",
            "                  [--global_attention {dot,general,mlp,none}]\n",
            "                  [--global_attention_function {softmax,sparsemax}]\n",
            "                  [--self_attn_type SELF_ATTN_TYPE]\n",
            "                  [--max_relative_positions MAX_RELATIVE_POSITIONS]\n",
            "                  [--heads HEADS] [--transformer_ff TRANSFORMER_FF]\n",
            "                  [--aan_useffn] [--lambda_align LAMBDA_ALIGN]\n",
            "                  [--alignment_layer ALIGNMENT_LAYER]\n",
            "                  [--alignment_heads ALIGNMENT_HEADS]\n",
            "                  [--full_context_alignment] [--copy_attn]\n",
            "                  [--copy_attn_type {dot,general,mlp,none}]\n",
            "                  [--generator_function {softmax,sparsemax}]\n",
            "                  [--copy_attn_force] [--reuse_copy_attn]\n",
            "                  [--copy_loss_by_seqlength] [--coverage_attn]\n",
            "                  [--lambda_coverage LAMBDA_COVERAGE]\n",
            "                  [--loss_scale LOSS_SCALE] [--apex_opt_level {O0,O1,O2,O3}]\n",
            "                  [--data_type DATA_TYPE] [--save_model SAVE_MODEL]\n",
            "                  [--save_checkpoint_steps SAVE_CHECKPOINT_STEPS]\n",
            "                  [--keep_checkpoint KEEP_CHECKPOINT]\n",
            "                  [--gpuid [GPUID [GPUID ...]]]\n",
            "                  [--gpu_ranks [GPU_RANKS [GPU_RANKS ...]]]\n",
            "                  [--world_size WORLD_SIZE] [--gpu_backend GPU_BACKEND]\n",
            "                  [--gpu_verbose_level GPU_VERBOSE_LEVEL]\n",
            "                  [--master_ip MASTER_IP] [--master_port MASTER_PORT]\n",
            "                  [--queue_size QUEUE_SIZE] [--seed SEED]\n",
            "                  [--param_init PARAM_INIT] [--param_init_glorot]\n",
            "                  [--train_from TRAIN_FROM]\n",
            "                  [--reset_optim {none,all,states,keep_states}]\n",
            "                  [--pre_word_vecs_enc PRE_WORD_VECS_ENC]\n",
            "                  [--pre_word_vecs_dec PRE_WORD_VECS_DEC]\n",
            "                  [--freeze_word_vecs_enc] [--freeze_word_vecs_dec]\n",
            "                  [--batch_size BATCH_SIZE]\n",
            "                  [--batch_size_multiple BATCH_SIZE_MULTIPLE]\n",
            "                  [--batch_type {sents,tokens}] [--pool_factor POOL_FACTOR]\n",
            "                  [--normalization {sents,tokens}]\n",
            "                  [--accum_count ACCUM_COUNT [ACCUM_COUNT ...]]\n",
            "                  [--accum_steps ACCUM_STEPS [ACCUM_STEPS ...]]\n",
            "                  [--valid_steps VALID_STEPS]\n",
            "                  [--valid_batch_size VALID_BATCH_SIZE]\n",
            "                  [--max_generator_batches MAX_GENERATOR_BATCHES]\n",
            "                  [--train_steps TRAIN_STEPS] [--single_pass]\n",
            "                  [--epochs EPOCHS] [--early_stopping EARLY_STOPPING]\n",
            "                  [--early_stopping_criteria [EARLY_STOPPING_CRITERIA [EARLY_STOPPING_CRITERIA ...]]]\n",
            "                  [--optim {sgd,adagrad,adadelta,adam,sparseadam,adafactor,fusedadam}]\n",
            "                  [--adagrad_accumulator_init ADAGRAD_ACCUMULATOR_INIT]\n",
            "                  [--max_grad_norm MAX_GRAD_NORM]\n",
            "                  [--dropout DROPOUT [DROPOUT ...]]\n",
            "                  [--attention_dropout ATTENTION_DROPOUT [ATTENTION_DROPOUT ...]]\n",
            "                  [--dropout_steps DROPOUT_STEPS [DROPOUT_STEPS ...]]\n",
            "                  [--truncated_decoder TRUNCATED_DECODER]\n",
            "                  [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]\n",
            "                  [--label_smoothing LABEL_SMOOTHING]\n",
            "                  [--average_decay AVERAGE_DECAY]\n",
            "                  [--average_every AVERAGE_EVERY]\n",
            "                  [--learning_rate LEARNING_RATE]\n",
            "                  [--learning_rate_decay LEARNING_RATE_DECAY]\n",
            "                  [--start_decay_steps START_DECAY_STEPS]\n",
            "                  [--decay_steps DECAY_STEPS]\n",
            "                  [--decay_method {noam,noamwd,rsqrt,none}]\n",
            "                  [--warmup_steps WARMUP_STEPS] [--log_file LOG_FILE]\n",
            "                  [--log_file_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET,50,40,30,20,10,0}]\n",
            "                  [--report_every REPORT_EVERY] [--exp_host EXP_HOST]\n",
            "                  [--exp EXP] [--tensorboard]\n",
            "                  [--tensorboard_log_dir TENSORBOARD_LOG_DIR]\n",
            "                  [-bucket_size BUCKET_SIZE]\n",
            "onmt_train: error: the following arguments are required: -src_vocab/--src_vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3mIBRoaWkcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d34c4fa-5e57-4648-d41a-c2a61a5f85f1"
      },
      "source": [
        "!onmt_translate -model save/_step_2000.pt \\\r\n",
        "                -src tok/limit-kyoto-test.en \\\r\n",
        "                -output tok/limit-kyoto-output.ja \\\r\n",
        "                -replace_unk -verbose"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_translate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/translate.py\", line 44, in main\n",
            "    translate(opt)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/translate.py\", line 15, in translate\n",
            "    translator = build_translator(opt, logger=logger, report_score=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/translate/translator.py\", line 32, in build_translator\n",
            "    fields, model, model_opt = load_test_model(opt)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/model_builder.py\", line 85, in load_test_model\n",
            "    map_location=lambda storage, loc: storage)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 571, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 229, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 210, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'save/_step_2000.pt'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}